{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Project - Gender Classificaton\n",
    "\n",
    "## Before you start\n",
    "\n",
    "You'll need the latest version of the **azureml-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
    "\n",
    "> **Note**:\n",
    "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azure-ai-ml\n",
      "Version: 1.13.0\n",
      "Summary: Microsoft Azure Machine Learning Client Library for Python\n",
      "Home-page: https://github.com/Azure/azure-sdk-for-python\n",
      "Author: Microsoft Corporation\n",
      "Author-email: azuresdkengsysadmins@microsoft.com\n",
      "License: MIT License\n",
      "Location: /anaconda/envs/azureml_py38/lib/python3.8/site-packages\n",
      "Requires: marshmallow, azure-storage-blob, pyyaml, azure-storage-file-datalake, azure-core, isodate, azure-mgmt-core, azure-common, jsonschema, msrest, pyjwt, strictyaml, colorama, opencensus-ext-azure, typing-extensions, azure-storage-file-share, tqdm, pydash\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your workspace\n",
    "\n",
    "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1663753569264
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Command Job with Custom Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tracking with MLflow\n",
    "\n",
    "When running a script as a job you can use MLflow in your training script to track the model. MLflow allows you to track any custom parameters, metrics, or artifacts you want to store with your job output.\n",
    "\n",
    "Run the following cells to create the **train-model-mlflow.py** script in the **src** folder. The script trains a classification model by using the **diabetes.csv** file in the same folder, which is passed as an argument. \n",
    "\n",
    "Review the code below to find that the script will import `mlflow` and log:\n",
    "\n",
    "- The regularization rate as a **parameter**. \n",
    "- The accuracy and AUC as **metrics**.\n",
    "- The plotted ROC curve as an **artifact**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train-model-mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train-model-mlflow.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # Normalize numerical data\n",
    "    norm_df = normalize_data(df)\n",
    "\n",
    "    # Encode categorical data\n",
    "    encod_df = encode_data(norm_df)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(encod_df)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to split the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X = df[[\"long_hair\",\"forehead_width_cm\",\"forehead_height_cm\",\"nose_wide\",\"nose_long\",\"lips_thin\",\"distance_nose_to_lip_long\"]].values\n",
    "    y = df['gender'].values\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to normalize numerical data\n",
    "def normalize_data(df):\n",
    "    print(\"Normalizing data...\")\n",
    "    numerical_columns = [\"forehead_width_cm\", \"forehead_height_cm\", \"distance_nose_to_lip_long\"]\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df\n",
    "\n",
    "# Function to label encode categorical data\n",
    "def encode_data(norm_df):\n",
    "    print(\"Encoding data...\")\n",
    "    categorical_columns = [\"long_hair\", \"nose_wide\", \"nose_long\", \"lips_thin\", \"gender\"]\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "       norm_df[col] = encoder.fit_transform(norm_df[col])\n",
    "    print(\"Encoded DataFrame:\")\n",
    "    print(norm_df.head())\n",
    "    return norm_df   \n",
    "\n",
    "# Function to train the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    print(\"Training model...\")\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    print(\"Evaluating model...\")\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    mlflow.log_metric(\"Accuracy\", acc)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_scores[:, 1])\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:, 1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "    mlflow.log_artifact(\"ROC-Curve.png\")\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can submit the script as a command job.\n",
    "\n",
    "Run the cell below to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/honest_drawer_gd5c4tkn12?wsid=/subscriptions/7784b486-c1df-4244-be68-cefd0413ea59/resourcegroups/rg-dp100-labs/workspaces/teseewfsd&tid=0c4563c3-3bc8-450f-8008-bc0edc7c121e\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# configure job\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python train-model-mlflow.py --training_data gender_classification_v7_2__2_.csv\",   #your dataset name within the folder\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"ismayilsiyad1\",#change to your computr cluster name\n",
    "    display_name=\"gender-train-mlflow\",\n",
    "    experiment_name=\"gender-training\", \n",
    "    tags={\"model_type\": \"LogisticRegression\"}\n",
    "    )\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Studio, navigate to the **diabetes-train-mlflow** job to explore the overview of the command job you ran:\n",
    "\n",
    "- Find the logged parameters in the **Overview** tab, under **Params**.\n",
    "- Find the logged metrics in the **Metrics** tab.\n",
    "- Find the logged artifacts in the **Images** tab (specifically for images), and in the **Outputs + logs** tab (all files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Command Job with Auto Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autologging with MLflow\n",
    "\n",
    "Instead of using custom logging, MLflow can also automatically log any parameters, metrics, and artifacts. Autologging with MLflow requires only one line of code.\n",
    "\n",
    "Run the following cell to create the **train-model-autolog.py** script in the **src** folder. The script trains a classification model by using the **diabetes.csv** file in the same folder, which is passed as an argument. \n",
    "\n",
    "Review the code below to find that the script will import `mlflow` and enables autologging with the line: \n",
    "\n",
    "`mlflow.autolog()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train-model-autolog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train-model-autolog.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # enable autologging\n",
    "    mlflow.autolog()\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # Normalize numerical data\n",
    "    norm_df = normalize_data(df)\n",
    "\n",
    "    # Encode categorical data\n",
    "    encod_df = encode_data(norm_df)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(encod_df)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to split the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X = df[[\"long_hair\",\"forehead_width_cm\",\"forehead_height_cm\",\"nose_wide\",\"nose_long\",\"lips_thin\",\"distance_nose_to_lip_long\"]].values\n",
    "    y = df['gender'].values\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to normalize numerical data\n",
    "def normalize_data(df):\n",
    "    print(\"Normalizing data...\")\n",
    "    numerical_columns = [\"forehead_width_cm\", \"forehead_height_cm\", \"distance_nose_to_lip_long\"]\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df\n",
    "\n",
    "# Function to label encode categorical data\n",
    "def encode_data(norm_df):\n",
    "    print(\"Encoding data...\")\n",
    "    categorical_columns = [\"long_hair\", \"nose_wide\", \"nose_long\", \"lips_thin\", \"gender\"]\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "       norm_df[col] = encoder.fit_transform(norm_df[col])\n",
    "    print(\"Encoded DataFrame:\")\n",
    "    print(norm_df.head())\n",
    "    return norm_df   \n",
    "\n",
    "# Function to train the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    print(\"Training model...\")\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    print(\"Evaluating model...\")\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_scores[:, 1])\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:, 1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "    \n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can submit the script as a command job.\n",
    "\n",
    "Run the cell below to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/eager_crayon_jjzkdjhtl5?wsid=/subscriptions/7784b486-c1df-4244-be68-cefd0413ea59/resourcegroups/rg-dp100-labs/workspaces/teseewfsd&tid=0c4563c3-3bc8-450f-8008-bc0edc7c121e\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# configure job\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python train-model-autolog.py --training_data gender_classification_v7_2__2_.csv\",\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"ismayilsiyad1\",\n",
    "    display_name=\"gender-train-autolog\",\n",
    "    experiment_name=\"gender-training\"\n",
    "    )\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Sweep Job for Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training script\n",
    "Hyperparameter tuning is ideal when you want to train a machine learning models but vary the input parameters. You'll need to create a training script that expects an input parameter representing one of the algorithm's hyperparameters.\n",
    "\n",
    "Run the following cells to create the **src** folder and the training script.\n",
    "\n",
    "Note that the training script expects two input parameters:\n",
    "\n",
    "- `--training_data` which expects a string. You'll specify the path to a registered data asset as the input training data.\n",
    "- `--reg_rate` which expects a number, but has a default value of `0.01`. You'll use this input parameter for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # Normalize numerical data\n",
    "    norm_df = normalize_data(df)\n",
    "\n",
    "    # Encode categorical data\n",
    "    encod_df = encode_data(norm_df)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(encod_df)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to split the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X = df[[\"long_hair\",\"forehead_width_cm\",\"forehead_height_cm\",\"nose_wide\",\"nose_long\",\"lips_thin\",\"distance_nose_to_lip_long\"]].values\n",
    "    y = df['gender'].values\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to normalize numerical data\n",
    "def normalize_data(df):\n",
    "    print(\"Normalizing data...\")\n",
    "    numerical_columns = [\"forehead_width_cm\", \"forehead_height_cm\", \"distance_nose_to_lip_long\"]\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df\n",
    "\n",
    "# Function to label encode categorical data\n",
    "def encode_data(norm_df):\n",
    "    print(\"Encoding data...\")\n",
    "    categorical_columns = [\"long_hair\", \"nose_wide\", \"nose_long\", \"lips_thin\", \"gender\"]\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "       norm_df[col] = encoder.fit_transform(norm_df[col])\n",
    "    print(\"Encoded DataFrame:\")\n",
    "    print(norm_df.head())\n",
    "    return norm_df   \n",
    "\n",
    "# Function to train the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    print(\"Training model...\")\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    print(\"Evaluating model...\")\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    mlflow.log_metric(\"Accuracy\", acc)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_scores[:, 1])\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:, 1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "    mlflow.log_artifact(\"ROC-Curve.png\")\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and run a command job\n",
    "\n",
    "Run the cell below to train a classification model to predict diabetes. The model is trained by running the **train\\.py** script that can be found in the **src** folder. It uses the registered `diabetes-data` data asset as the training data. \n",
    "\n",
    "- `code`: specifies the folder that includes the script to run.\n",
    "- `command`: specifies what to run exactly.\n",
    "- `environment`: specifies the necessary packages to be installed on the compute before running the command.\n",
    "- `compute`: specifies the compute to use to run the command.\n",
    "- `display_name`: the name of the individual job.\n",
    "- `experiment_name`: the name of the experiment the job belongs to.\n",
    "\n",
    "Note that the command job only runs the training script once, with a regularization rate of `0.1`. Before you run a sweep job to tune hyperparameters, it's a best practice to test whether your script works as expected with a command job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1667592538771
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/epic_bell_9gb16gjrrb?wsid=/subscriptions/7784b486-c1df-4244-be68-cefd0413ea59/resourcegroups/rg-dp100-labs/workspaces/teseewfsd&tid=0c4563c3-3bc8-450f-8008-bc0edc7c121e\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# configure job\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python train.py --training_data ${{inputs.gender_data}} --reg_rate ${{inputs.reg_rate}}\",\n",
    "    inputs={\n",
    "        \"gender_data\": Input(\n",
    "            type=AssetTypes.URI_FILE, \n",
    "            path=\"azureml:genders-data:1\"\n",
    "            ),\n",
    "        \"reg_rate\": 0.01,\n",
    "    },\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"ismayilsiyad1\",\n",
    "    display_name=\"gender-train-mlflow\",\n",
    "    experiment_name=\"gender-training\", \n",
    "    tags={\"model_type\": \"LogisticRegression\"}\n",
    "    )\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Define the search space\n",
    "\n",
    "When your command job has completed successfully, you can configure and run a sweep job. \n",
    "\n",
    "First, you'll need to specify the search space for your hyperparameter. To train three models, each with a different regularization rate (`0.01`, `0.1`, or `1`), you can define the search space with a `Choice` hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1667592546442
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.sweep import Choice\n",
    "\n",
    "command_job_for_sweep = job(\n",
    "    reg_rate=Choice(values=[0.01, 0.1, 1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and submit the sweep job\n",
    "\n",
    "You'll use the sweep function to do hyperparameter tuning on your training script. To configure a sweep job, you'll need to configure the following:\n",
    "\n",
    "- `compute`: Name of the compute target to execute the job on.\n",
    "- `sampling_algorithm`: The hyperparameter sampling algorithm to use over the search space. Allowed values are `random`, `grid` and `bayesian`.\n",
    "- `primary_metric`: The name of the primary metric reported by each trial job. The metric must be logged in the user's training script using `mlflow.log_metric()` with the same corresponding metric name.\n",
    "- `goal`: The optimization goal of the `primary_metric`. The allowed values are `maximize` and `minimize`.\n",
    "- `limits`: Limits for the sweep job. For example, the maximum amount of trials or models you want to train.\n",
    "\n",
    "Note that the command job is used as the base for the sweep job. The configuration for the command job will be reused by the sweep job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1667592681475
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# apply the sweep parameter to obtain the sweep_job\n",
    "sweep_job = command_job_for_sweep.sweep(\n",
    "    compute=\"ismayilsiyad1\",\n",
    "    sampling_algorithm=\"grid\",\n",
    "    primary_metric=\"training_accuracy_score\",\n",
    "    goal=\"Maximize\",\n",
    ")\n",
    "\n",
    "# set the name of the sweep job experiment\n",
    "sweep_job.experiment_name=\"sweep-gender\"\n",
    "\n",
    "# define the limits for this sweep\n",
    "sweep_job.set_limits(max_total_trials=2, max_concurrent_trials=1, timeout=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to submit the sweep job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1667592716881
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/orange_dress_rv62rc6l6c?wsid=/subscriptions/7784b486-c1df-4244-be68-cefd0413ea59/resourcegroups/rg-dp100-labs/workspaces/teseewfsd&tid=0c4563c3-3bc8-450f-8008-bc0edc7c121e\n"
     ]
    }
   ],
   "source": [
    "returned_sweep_job = ml_client.create_or_update(sweep_job)\n",
    "aml_url = returned_sweep_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of Sweep Job, and verify which hyper parameter value (Regularization) is giving the best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's check AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Configure AutoML Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "You don't need to create a training script for automated machine learning, but you do need to load the training data. \n",
    "\n",
    "In this case, you'll use a dataset containing details of diabetes patients. \n",
    "\n",
    "To pass a dataset as an input to an automated machine learning job, the data must be in tabular form and include a target column. For the data to be interpreted as a tabular dataset, the input dataset must be a **MLTable**.\n",
    "\n",
    "A MLTable data asset has already been created for you during set-up. You can explore the data asset by navigating to the **Data** page. You'll retrieve the data asset here by specifying its name `diabetes-training-table` and version `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1664965655212
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "# creates a dataset based on the files in the local data folder\n",
    "my_training_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml:gender_table:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure automated machine learning job\n",
    "\n",
    "Now, you're ready to configure the automated machine learning experiment.\n",
    "\n",
    "When you run the code below, it will create an automated machine learning job that:\n",
    "\n",
    "- Uses the compute cluster named `aml-cluster`\n",
    "- Sets `Diabetic` as the target column\n",
    "- Sets `accuracy` as the primary metric\n",
    "- Times out after `60` minutes of total training time \n",
    "- Trains a maximum of `5` models\n",
    "- No model will be trained with the `LogisticRegression` algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1664965734226
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import automl\n",
    "\n",
    "# configure the classification job\n",
    "classification_job = automl.classification(\n",
    "    compute=\"ccluster\",\n",
    "    experiment_name=\"auto-ml-class-dev\",\n",
    "    training_data=my_training_data_input,\n",
    "    target_column_name=\"gender\",\n",
    "    primary_metric=\"accuracy\",\n",
    "    n_cross_validations=2,\n",
    "    enable_model_explainability=True\n",
    ")\n",
    "\n",
    "# set the limits (optional)\n",
    "classification_job.set_limits(\n",
    "    timeout_minutes=30, \n",
    "    trial_timeout_minutes=20, \n",
    "    max_trials=4,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# set the training properties (optional)\n",
    "classification_job.set_training(\n",
    "    blocked_training_algorithms=[\"LogisticRegression\"], \n",
    "    enable_onnx_compatible_models=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an automated machine learning job\n",
    "\n",
    "OK, you're ready to go. Let's run the automated machine learning experiment.\n",
    "\n",
    "> **Note**: This may take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1664965739262
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/lime_yam_906gtgj0k2?wsid=/subscriptions/7784b486-c1df-4244-be68-cefd0413ea59/resourcegroups/rg-dp100-labs/workspaces/teseewfsd&tid=0c4563c3-3bc8-450f-8008-bc0edc7c121e\n"
     ]
    }
   ],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    classification_job\n",
    ")  \n",
    "\n",
    "# submit the job to the backend\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
